{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L7T3iieSjpN"
      },
      "source": [
        "# InkPulse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykEw47G_S3Tq"
      },
      "source": [
        "## Import Dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "8UuB9bkkRvvJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "import openai\n",
        "import random\n",
        "random.seed(23)\n",
        "import csv\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Vf4tOrTSl_"
      },
      "source": [
        "## Set Parameters(dataset name, open ai key and path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "jAMlul-FTWMX"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"legislation_formal_study\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "OKRHUw8tTbT7"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "k4f94Dc7al85"
      },
      "outputs": [],
      "source": [
        "# # In Colab\n",
        "# import zipfile\n",
        "# import shutil\n",
        "# script_dir = os.getcwd()\n",
        "# static_dir = script_dir\n",
        "\n",
        "# # Reading dataset\n",
        "# zip_path = f\"{dataset_name}.zip\"\n",
        "# import_data_dir = \"/content/import_dataset\"\n",
        "# csv_path = f\"{dataset_name}.csv\"\n",
        "# final_extract_path = os.path.join(import_data_dir, dataset_name)\n",
        "\n",
        "# if os.path.exists(final_extract_path):\n",
        "#     shutil.rmtree(final_extract_path)\n",
        "# os.makedirs(import_data_dir, exist_ok=True)\n",
        "\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     members = zip_ref.namelist()\n",
        "#     top_level_dirs = set(m.split('/')[0] for m in members if '/' in m)\n",
        "#     if len(top_level_dirs) == 1:\n",
        "#         top_dir = list(top_level_dirs)[0]\n",
        "#         for member in members:\n",
        "#             relative_path = member[len(top_dir)+1:] if member.startswith(top_dir + '/') else member\n",
        "#             if relative_path:\n",
        "#                 target_path = os.path.join(final_extract_path, relative_path)\n",
        "#                 os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "#                 with zip_ref.open(member) as source, open(target_path, 'wb') as target:\n",
        "#                     target.write(source.read())\n",
        "#     else:\n",
        "#         zip_ref.extractall(final_extract_path)\n",
        "# if os.path.exists(csv_path):\n",
        "#     target_csv_path = os.path.join(import_data_dir, f\"{dataset_name}.csv\")\n",
        "#     shutil.move(csv_path, target_csv_path)\n",
        "\n",
        "# Local\n",
        "script_dir = os.getcwd()\n",
        "static_dir = os.path.dirname(script_dir)\n",
        "csv_path = os.path.join(static_dir, \"import_dataset\", f\"{dataset_name}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "iK50ZdX26E6C"
      },
      "outputs": [],
      "source": [
        "def path_exists(path):\n",
        "    if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "      print(f\"Create folder in {path}\")\n",
        "    else:\n",
        "      print(f\"Folder already exist in {path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjFEEKY-5wCL",
        "outputId": "7cf2ad99-4459-4dfd-9df5-29cae17fe244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder already exist in d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset\\legislation_formal_study.\n",
            "Folder already exist in d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset/legislation_formal_study/json.\n",
            "Folder already exist in d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset/legislation_formal_study/segment.\n",
            "Folder already exist in d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset\\legislation_formal_study\\segment_results.\n"
          ]
        }
      ],
      "source": [
        "import_data_dir = os.path.join(static_dir, \"import_dataset\")\n",
        "json_path = os.path.join(static_dir, \"import_dataset\", f\"{dataset_name}\")\n",
        "session_id_collection = []\n",
        "for filename in os.listdir(json_path):\n",
        "    filename = filename.removesuffix(\".jsonl\")\n",
        "    session_id_collection.append(filename)\n",
        "\n",
        "new_path = os.path.join(static_dir, \"dataset\", f\"{dataset_name}\")\n",
        "path_exists(new_path)\n",
        "\n",
        "new_json_path = os.path.join(static_dir, f\"dataset/{dataset_name}/json\")\n",
        "path_exists(new_json_path)\n",
        "\n",
        "new_segment_path = os.path.join(static_dir, f\"dataset/{dataset_name}/segment\")\n",
        "path_exists(new_segment_path)\n",
        "\n",
        "new_segment_results_path = os.path.join(static_dir, \"dataset\", f\"{dataset_name}\", \"segment_results\")\n",
        "path_exists(new_segment_results_path)\n",
        "\n",
        "fine_file_path = os.path.join(static_dir, \"dataset\", f\"{dataset_name}\", f\"fine.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R6wi9CSZmy_"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7-IJMM7a47E"
      },
      "source": [
        "### Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "1SJXqpvWLXwH"
      },
      "outputs": [],
      "source": [
        "def load_json(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    return data\n",
        "\n",
        "def write_json(data, file_path, session):\n",
        "    actual_session = session+'.jsonl'\n",
        "    new_file_path = os.path.join(file_path, actual_session)\n",
        "    with open(new_file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Data written to {new_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "06BwQG7sa-vI"
      },
      "outputs": [],
      "source": [
        "def collect_data(snapshots):\n",
        "    segments = []\n",
        "    current_text = \"\"\n",
        "    current_source = None\n",
        "    current_start_time = None\n",
        "    current_end_time = None\n",
        "    last_event_time = None\n",
        "\n",
        "    for snap in snapshots:\n",
        "        text = snap['text']\n",
        "        source = snap['eventSource']\n",
        "        event_time = snap['event_time']\n",
        "        event_name = snap['eventName']\n",
        "\n",
        "        if current_source is None:\n",
        "            current_source = source\n",
        "            current_start_time = event_time\n",
        "        if source != current_source or event_name == \"suggestion-open\":\n",
        "            if current_text:\n",
        "                segments.append({\n",
        "                    \"text\": current_text,\n",
        "                    \"source\": current_source,\n",
        "                    \"start_time\": current_start_time,\n",
        "                    \"end_time\": current_end_time,\n",
        "                    \"last_event_time\": last_event_time\n",
        "                })\n",
        "            current_text = text\n",
        "            if event_name != \"suggestion-open\":\n",
        "                current_source = source\n",
        "            current_start_time = event_time\n",
        "        else:\n",
        "            current_text = text\n",
        "\n",
        "        current_end_time = event_time\n",
        "        last_event_time = event_time\n",
        "\n",
        "    if current_text:\n",
        "        segments.append({\n",
        "            \"text\": current_text,\n",
        "            \"source\": current_source,\n",
        "            \"start_time\": current_start_time,\n",
        "            \"end_time\": current_end_time,\n",
        "            \"last_event_time\": last_event_time\n",
        "        })\n",
        "\n",
        "    return segments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcVl1k4-bHgT"
      },
      "source": [
        "### Calculate progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "qUg1Gow1bTD8"
      },
      "outputs": [],
      "source": [
        "def convert_and_clean(data, delta):\n",
        "    total_length = len(data[-1]['text'])\n",
        "    current_progress = 0\n",
        "    for entry in data:\n",
        "        entry['start_progress'] = current_progress\n",
        "        entry['end_progress'] = len(entry['text']) / total_length\n",
        "        current_progress = entry['end_progress']\n",
        "    base_time_str = data[0]['start_time']\n",
        "    base_time = datetime.strptime(base_time_str, \"%Y-%m-%d %H:%M:%S\")\n",
        "    for event in data:\n",
        "      if 'start_time' in event:\n",
        "          start_time = datetime.strptime(event['start_time'], \"%Y-%m-%d %H:%M:%S\")\n",
        "          event['start_time'] = (start_time - base_time).total_seconds()\n",
        "      if 'end_time' in event:\n",
        "          end_time = datetime.strptime(event['end_time'], \"%Y-%m-%d %H:%M:%S\")\n",
        "          event['end_time'] = (end_time - base_time).total_seconds()\n",
        "      if 'last_event_time' in event:\n",
        "          last_event_time = datetime.strptime(event['last_event_time'], \"%Y-%m-%d %H:%M:%S\")\n",
        "          event['last_event_time'] = (last_event_time - base_time).total_seconds()\n",
        "    filtered_sentences = []\n",
        "    for i, entry in enumerate(data):\n",
        "      text = entry.get(\"text\", \"\").strip()\n",
        "      if text == \"\":\n",
        "          continue\n",
        "      if filtered_sentences:\n",
        "          prev_text = filtered_sentences[-1][\"text\"]\n",
        "          delta_chars = sum(1 for a, b in zip(prev_text, text) if a != b) + abs(len(prev_text) - len(text))\n",
        "          if delta_chars < delta:\n",
        "              continue\n",
        "      filtered_sentences.append(entry)\n",
        "      data = filtered_sentences\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_and_calculate(data):\n",
        "    info = data[\"info\"]\n",
        "    total_length = len(data['text'][0])\n",
        "    for i in info:\n",
        "        i['progress'] = len(i['current_text']) / total_length\n",
        "        i.pop('current_text', None)\n",
        "        \n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8DJB3UsaE9o"
      },
      "source": [
        "### Reconstruct text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "4hLRT9b3aG7M"
      },
      "outputs": [],
      "source": [
        "info_data = []\n",
        "sentence_data = []\n",
        "def get_data(dataset_name, session_id_collection, static_dir, is_json):\n",
        "    if is_json:\n",
        "        json_path = os.path.join(static_dir, f\"dataset/{dataset_name}/json\")\n",
        "    else:\n",
        "        json_path = os.path.join(static_dir, f\"dataset/{dataset_name}/segment\")\n",
        "    for session in session_id_collection:\n",
        "        extracted_data = {'init_text': [], 'init_time': [], 'json': [], 'text': [], 'info': [], 'end_time': [], 'snapshots': []}\n",
        "        file_path = os.path.join(static_dir, \"import_dataset\", f\"{dataset_name}\")\n",
        "        actual_session = session + '.jsonl'\n",
        "        new_file_path = os.path.join(file_path, actual_session)\n",
        "        with open(new_file_path, 'r', encoding='utf-8') as file:\n",
        "            for line_number, line in enumerate(file, start=1):\n",
        "                cleaned_line = line.replace('\\0', '')\n",
        "                if cleaned_line.strip():\n",
        "                    json_data = json.loads(cleaned_line)\n",
        "                    if line_number == 1:\n",
        "                        init_text = json_data.get('currentDoc', '')\n",
        "                        init_timestamp = json_data.get('eventTimestamp')\n",
        "                        init_time = datetime.fromtimestamp(init_timestamp / 1000).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    event_num = json_data.get('eventNum')\n",
        "                    event_name = json_data.get('eventName')\n",
        "                    event_source = json_data.get('eventSource')\n",
        "                    event_timestamp = json_data.get('eventTimestamp')\n",
        "                    event_time = datetime.fromtimestamp(event_timestamp / 1000).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    last_event_time = event_time\n",
        "                    text_delta = json_data.get('textDelta', {})\n",
        "                    current_suggestions = json_data.get('currentSuggestions', {})\n",
        "                    entry = {'eventNum': event_num, 'eventName': event_name, 'eventSource': event_source, 'event_time': event_time, 'textDelta': text_delta, 'currentSuggestions': current_suggestions}\n",
        "                    extracted_data['json'].append(entry)\n",
        "        extracted_data['init_time'].append(init_time)\n",
        "        extracted_data['init_text'].append(init_text)\n",
        "        text = ''.join(extracted_data['init_text'])\n",
        "        previous_event_name = None\n",
        "        if text != \"\" and text != \"\\n\":\n",
        "            extracted_data['snapshots'].append({\n",
        "                'text': text,\n",
        "                'eventName': '',\n",
        "                'eventSource': 'api',\n",
        "                'event_time': init_time,\n",
        "                'eventNum': 0\n",
        "            })\n",
        "        for entry in extracted_data['json']:\n",
        "            text_delta = entry.get('textDelta', {})\n",
        "            if not isinstance(text_delta, dict):\n",
        "                if isinstance(text_delta, str) and text_delta.strip():\n",
        "                    try:\n",
        "                        text_delta = json.loads(text_delta)\n",
        "                    except json.JSONDecodeError:\n",
        "                        text_delta = {}\n",
        "                else:\n",
        "                    text_delta = {}\n",
        "            ops = text_delta.get('ops', [])\n",
        "            event_name = entry.get('eventName')\n",
        "            event_source = entry.get('eventSource', 'unknown')\n",
        "            event_time = entry.get('event_time')\n",
        "            event_num = entry.get('eventNum')\n",
        "            pos = entry.get('currentCursor', 0)\n",
        "            for op in ops:\n",
        "                if 'retain' in op:\n",
        "                    pos += op['retain']\n",
        "                elif 'insert' in op:\n",
        "                    inserts = op['insert']\n",
        "                    if not isinstance(inserts, str):\n",
        "                        # print(f\"skip image insert: {inserts}\")\n",
        "                        continue\n",
        "                    source = event_source\n",
        "                    if previous_event_name == \"suggestion-close\" and len(inserts) > 5:\n",
        "                        source = \"api\"\n",
        "                    text = text[:pos] + inserts + text[pos:]\n",
        "                    extracted_data['info'].append({\n",
        "                        'id': event_num,\n",
        "                        'name': 'text-insert',\n",
        "                        'text': inserts,\n",
        "                        'eventSource': source,\n",
        "                        'event_time': event_time,\n",
        "                        'count': len(inserts),\n",
        "                        'pos': pos,\n",
        "                        'current_text': text,\n",
        "                    })\n",
        "                    pos += len(inserts)\n",
        "                elif 'delete' in op:\n",
        "                    delete_count = op['delete']\n",
        "                    deleted_text = text[pos:pos + delete_count]\n",
        "                    text = text[:pos] + text[pos + delete_count:]\n",
        "                    extracted_data['info'].append({\n",
        "                        'id': event_num,\n",
        "                        'name': 'text-delete',\n",
        "                        'text': deleted_text,\n",
        "                        'eventSource': event_source,\n",
        "                        'event_time': event_time,\n",
        "                        'count': delete_count,\n",
        "                        'pos': pos,\n",
        "                        'current_text': text,\n",
        "                    })\n",
        "            if event_name == 'suggestion-open':\n",
        "                extracted_data['info'].append({\n",
        "                    'id': event_num,\n",
        "                    'name': event_name,\n",
        "                    'eventSource': event_source,\n",
        "                    'event_time': event_time,\n",
        "                    'current_text': text,\n",
        "                })\n",
        "                extracted_data['snapshots'].append({\n",
        "                    'text': text,\n",
        "                    'eventName': event_name,\n",
        "                    'eventSource': event_source,\n",
        "                    'event_time': event_time,\n",
        "                    'eventNum': event_num\n",
        "                })\n",
        "            if ops:\n",
        "                extracted_data['snapshots'].append({\n",
        "                    'text': text,\n",
        "                    'eventName': event_name,\n",
        "                    'eventSource': source,\n",
        "                    'event_time': event_time,\n",
        "                    'eventNum': event_num\n",
        "                })\n",
        "            previous_event_name = event_name\n",
        "        if entry['eventNum'] == None:\n",
        "            for entry in extracted_data['info']:\n",
        "                if 'id' in entry:\n",
        "                    del entry['id']\n",
        "        # print(text)\n",
        "        data = collect_data(extracted_data['snapshots'])\n",
        "        data = convert_and_clean(data, delta = 5)\n",
        "        extracted_data['text'].append(text)\n",
        "        extracted_data['end_time'] = last_event_time\n",
        "        extracted_data = convert_and_calculate(extracted_data)\n",
        "        extracted_data.pop('json', None)\n",
        "        extracted_data.pop('snapshots', None)\n",
        "        if is_json:\n",
        "            write_json(extracted_data, json_path, session)\n",
        "        else:\n",
        "            write_json(data, json_path, session)\n",
        "        info_data.append(extracted_data)\n",
        "        sentence_data.append(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-82vmuTbms2"
      },
      "source": [
        "## Data Preprocessing - Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrRjTVfYbxm3",
        "outputId": "090051e7-f955-4945-e96b-7aed192c922b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data written to d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset/legislation_formal_study/json\\0c44adf9178a443a9fd7a5a2edaeb7c4.jsonl\n",
            "Data written to d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset/legislation_formal_study/json\\2c4b5bda6f6f43ab9ffc990e7cae9693.jsonl\n",
            "Data written to d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset/legislation_formal_study/json\\6df918d72046461a98d275bf3fac31d0.jsonl\n",
            "Data written to d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset/legislation_formal_study/segment\\0c44adf9178a443a9fd7a5a2edaeb7c4.jsonl\n",
            "Data written to d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset/legislation_formal_study/segment\\2c4b5bda6f6f43ab9ffc990e7cae9693.jsonl\n",
            "Data written to d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset/legislation_formal_study/segment\\6df918d72046461a98d275bf3fac31d0.jsonl\n"
          ]
        }
      ],
      "source": [
        "get_data(dataset_name, session_id_collection, static_dir, is_json=True)\n",
        "get_data(dataset_name, session_id_collection, static_dir, is_json=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6xzB8LIx_HH"
      },
      "source": [
        "### Check Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wTE3iecyCfj",
        "outputId": "440f616d-7ca7-4155-f0a1-241d9a78d8a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "if info_data[0][\"text\"][0] == sentence_data[0][-1][\"text\"]:\n",
        "    print(\"True\")\n",
        "else:\n",
        "    print(\"False\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcrFs-tO9FT9"
      },
      "source": [
        "## Calculate Semantic Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "qKIvyu1pIWhr"
      },
      "outputs": [],
      "source": [
        "def read_sentences(file_path):\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "    sentences = [\n",
        "      {\n",
        "          \"text\": entry[\"text\"],\n",
        "          \"source\": entry.get(\"source\", \"unknown\"),\n",
        "          \"start_progress\": entry[\"start_progress\"],\n",
        "          \"end_progress\": entry[\"end_progress\"],\n",
        "          \"start_time\": entry[\"start_time\"],\n",
        "          \"end_time\": entry[\"end_time\"],\n",
        "          \"last_event_time\": entry[\"last_event_time\"]\n",
        "      }\n",
        "      for entry in data if \"text\" in entry\n",
        "    ]\n",
        "  return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "8R6Ycp0iIckj"
      },
      "outputs": [],
      "source": [
        "client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
        "  response = client.embeddings.create(input=[text], model=model)\n",
        "  return np.array(response.data[0].embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "DvumCxNrIofz"
      },
      "outputs": [],
      "source": [
        "def compute_vector_norm(residual_vector):\n",
        "  return float(np.linalg.norm(residual_vector))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "YD3do-zYI0Qh"
      },
      "outputs": [],
      "source": [
        "def analyze_residuals(sentences, check):\n",
        "  results = []\n",
        "  delta = 5\n",
        "  first_is_empty = not check[\"init_text\"] or check[\"init_text\"][0].strip() == \"\"\n",
        "\n",
        "  for i, sentence in enumerate(sentences):\n",
        "      text = sentence.get(\"text\", \"\").strip()\n",
        "\n",
        "      if first_is_empty:\n",
        "          if i == 0:\n",
        "              sentence[\"residual_vector\"] = 0.0\n",
        "              continue\n",
        "          elif i == 1:\n",
        "              sentence[\"embedding\"] = get_openai_embedding(text)\n",
        "              sentence[\"residual_vector\"] = 1.0\n",
        "              continue\n",
        "\n",
        "      if i == 0:\n",
        "          sentence[\"embedding\"] = get_openai_embedding(text)\n",
        "          sentence[\"residual_vector\"] = 0.0\n",
        "      else:\n",
        "          prev_text = sentences[i - 1][\"text\"]\n",
        "          delta_chars = sum(1 for a, b in zip(prev_text, text) if a != b) + abs(len(prev_text) - len(text))\n",
        "          sentence[\"embedding\"] = get_openai_embedding(text)\n",
        "          if delta_chars <= delta:\n",
        "              sentence[\"residual_vector\"] = 0.0\n",
        "          else:\n",
        "              prev_embedding = sentences[i - 1][\"embedding\"]\n",
        "              residual_vector = sentence[\"embedding\"] - prev_embedding\n",
        "              sentence[\"residual_vector\"] = compute_vector_norm(residual_vector)\n",
        "  norms = [s[\"residual_vector\"] for s in sentences]\n",
        "  min_norm = min(norms)\n",
        "  max_norm = max(norms)\n",
        "  norm_range = max_norm - min_norm if max_norm != min_norm else 1.0\n",
        "  for sentence in sentences:\n",
        "      raw_norm = sentence[\"residual_vector\"]\n",
        "      normalized = (raw_norm - min_norm) / norm_range\n",
        "      sentence[\"residual_vector_norm\"] = normalized\n",
        "  for sentence in sentences:\n",
        "      result_entry = {\n",
        "          \"sentence\": sentence[\"text\"],\n",
        "          \"source\": sentence[\"source\"],\n",
        "          \"start_progress\": sentence[\"start_progress\"],\n",
        "          \"end_progress\": sentence[\"end_progress\"],\n",
        "          \"start_time\": sentence[\"start_time\"],\n",
        "          \"end_time\": sentence[\"end_time\"],\n",
        "          \"last_event_time\": sentence[\"last_event_time\"],\n",
        "          \"residual_vector\": sentence[\"residual_vector\"],\n",
        "          \"residual_vector_norm\": sentence[\"residual_vector_norm\"],\n",
        "      }\n",
        "      results.append(result_entry)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "A7bYZBG4I9tO"
      },
      "outputs": [],
      "source": [
        "def convert_types(obj):\n",
        "  if isinstance(obj, (np.float32, np.float64)):\n",
        "      return float(obj)\n",
        "  elif isinstance(obj, (np.int32, np.int64)):\n",
        "      return int(obj)\n",
        "  elif isinstance(obj, np.ndarray):\n",
        "      return obj.tolist()\n",
        "  return obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "Wv37wryBJCRQ"
      },
      "outputs": [],
      "source": [
        "def save_results_to_json(results, session_id, output_dir):\n",
        "  output_file = os.path.join(output_dir, f\"{session_id}.json\")\n",
        "  results_converted = [\n",
        "      {k: convert_types(v) for k, v in entry.items()}\n",
        "      for entry in results\n",
        "  ]\n",
        "  with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "      json.dump(results_converted, f, indent=4, ensure_ascii=False)\n",
        "  print(f\"Segment results saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW9QJhodIVBO"
      },
      "source": [
        "## Calculate Semantic Score - Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAnagCA19IUg",
        "outputId": "fb067c4d-17f5-4116-9838-9cd5ef564fc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Segment results saved to d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset\\legislation_formal_study\\segment_results\\0c44adf9178a443a9fd7a5a2edaeb7c4.json\n",
            "Segment results saved to d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset\\legislation_formal_study\\segment_results\\2c4b5bda6f6f43ab9ffc990e7cae9693.json\n",
            "Segment results saved to d:\\Study\\Lab\\Vitualization\\Ink-Pulse\\static\\dataset\\legislation_formal_study\\segment_results\\6df918d72046461a98d275bf3fac31d0.json\n"
          ]
        }
      ],
      "source": [
        "for file_name in os.listdir(new_segment_path):\n",
        "  if file_name.endswith(\".jsonl\"):\n",
        "      file_path = os.path.join(new_segment_path, file_name)\n",
        "      session_id = os.path.splitext(file_name)[0]\n",
        "      sentences = read_sentences(file_path)\n",
        "      check_file = os.path.join(new_json_path, session_id + \".jsonl\")\n",
        "      check = load_json(check_file)\n",
        "      results = analyze_residuals(sentences, check)\n",
        "      save_results_to_json(results, session_id, new_segment_results_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xcAK-QyJm3Z"
      },
      "source": [
        "## Calculate Text Quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "lbqS7_ESJ6Zz"
      },
      "outputs": [],
      "source": [
        "def chatgpt_prompter(input_prompt):\n",
        "    completion = client.chat.completions.create(\n",
        "        # model=\"gpt-3.5-turbo\",\n",
        "        model = \"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": input_prompt}\n",
        "        ],\n",
        "        temperature = 0,\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "vjt19iMKJ_eU"
      },
      "outputs": [],
      "source": [
        "def process_evaluate(answer):\n",
        "    answer = answer.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
        "    if answer.startswith(\"[\") and answer.endswith(\"]\"):\n",
        "        answer = \"{\" + answer[1:-1] + \"}\"\n",
        "    try:\n",
        "        data = json.loads(answer)\n",
        "        idea_score = data[\"idea_score\"]\n",
        "        coherence_score = data[\"coherence_score\"]\n",
        "        score = int (idea_score) + int(coherence_score)\n",
        "    except Exception as e:\n",
        "        print(\"Fail:\", answer)\n",
        "        print(e)\n",
        "        score = 0\n",
        "        with open(\"failed.txt\", \"a\") as f:\n",
        "            f.write(answer + \"\\n\")\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "4ENcFWqtKH3C"
      },
      "outputs": [],
      "source": [
        "def longest_common(s1, s2):\n",
        "    min_len = min(len(s1), len(s2))\n",
        "    i = 0\n",
        "    while i < min_len and s1[i] == s2[i]:\n",
        "        i += 1\n",
        "    return s1[:i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "h4wjs47AKGAT"
      },
      "outputs": [],
      "source": [
        "def read_sentences(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "        if \"init_text\" in data and data[\"init_text\"] and data[\"init_text\"][0].strip():\n",
        "            intro = data[\"init_text\"][0].strip()\n",
        "            article = data[\"text\"][0].lstrip()\n",
        "            prefix = longest_common(intro, article)\n",
        "            if prefix:\n",
        "                article = article[len(prefix):].lstrip()\n",
        "        else:\n",
        "            intro = \"\"\n",
        "            article = data[\"text\"][0].lstrip()\n",
        "\n",
        "    return intro, article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "dKKZQIS1J72W"
      },
      "outputs": [],
      "source": [
        "def evaluate_prompt(session_id, topic, intro, article):\n",
        "    result = []\n",
        "    EVALUATION_PROMPT_TEMPLATE = f\"\"\"\n",
        "        Topic: {topic}\n",
        "        Introduction: {intro}\n",
        "        Article: {article}\n",
        "        —–\n",
        "        You are evaluating an article co-written by a human and an AI. You must objectively score the topic-article pair based on the two criteria below:\n",
        "        New Idea (0-5):\n",
        "        Evaluate how much *new, original thinking* the article contributes *beyond the Introduction*.\n",
        "        - 0: No ideas; incoherent or irrelevant.\n",
        "        - 1: Purely obvious or generic.\n",
        "        - 2: Fragmented or shallow ideas.\n",
        "        - 3: Standard ideas with some development.\n",
        "        - 4: Clear new insights or novel angles.\n",
        "        - 5: Multiple strong original ideas; deep or creative expansion beyond the intro.\n",
        "\n",
        "        Coherence (0-5):\n",
        "        Evaluate how well the article maintains *logical structure*, *natural transitions*, and *stylistic consistency* throughout.\n",
        "        - 0: Disjointed or jarring; abrupt shifts in tone or topic.\n",
        "        - 1: Minimal cohesion; sections feel stitched together.\n",
        "        - 2: Mostly smooth but has several awkward transitions.\n",
        "        - 3: Generally coherent; occasional unevenness.\n",
        "        - 4: Well-structured with clear, natural progression.\n",
        "        - 5: Seamless flow and unity; stylistically and structurally refined.\n",
        "\n",
        "        Note: The Introduction is provided for context only and should NOT be considered part of the article content for scoring purposes.\n",
        "        Your response should be in JSON format as follows:\n",
        "        [\"session_id\": {session_id}, \"idea_score\": \"idea_score\",\"coherence_score\": \"coherence_score\", \"reason\": \"Explain briefly why you gave these scores, citing specific examples or patterns from the article.\"]\n",
        "        —–\n",
        "        score:\n",
        "    \"\"\"\n",
        "    answer = chatgpt_prompter(EVALUATION_PROMPT_TEMPLATE)\n",
        "    # print(\"system_prompt: \", EVALUATION_PROMPT_TEMPLATE)\n",
        "    # print(answer)\n",
        "    score = process_evaluate(answer)\n",
        "    # print(score)\n",
        "    result.append({\n",
        "        'Prompt': EVALUATION_PROMPT_TEMPLATE,\n",
        "        'Evaluation': answer,\n",
        "        'Score': score\n",
        "    })\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmDuMuFtKUq5",
        "outputId": "72a4981e-1d3a-4cd6-d67b-f3b55a077b0c"
      },
      "outputs": [],
      "source": [
        "topic_dir = os.path.join(import_data_dir, f\"{dataset_name}.csv\")\n",
        "\n",
        "judge_score = []\n",
        "topic_df = pd.read_csv(topic_dir)\n",
        "for file_name in os.listdir(new_json_path):\n",
        "    if file_name.endswith(\".jsonl\"):\n",
        "        file_path = os.path.join(new_json_path, file_name)\n",
        "        session_id = os.path.splitext(file_name)[0]\n",
        "        session = topic_df[topic_df[\"session_id\"] == session_id]\n",
        "        if not session.empty:\n",
        "            topic = session[\"prompt_code\"].values[0]\n",
        "            intro, article = read_sentences(file_path)\n",
        "            result = evaluate_prompt(session_id, topic, intro, article)\n",
        "            for item in result:\n",
        "              item[\"score\"] = result[0][\"Score\"]\n",
        "            judge_score.append({\n",
        "                \"session_id\": session_id,\n",
        "                \"judge_score\": result[0]\n",
        "            })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy7t1OyYMjPr"
      },
      "source": [
        "## Clean Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdMP2RP_M06J"
      },
      "source": [
        "Convert the sentence into length of the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "XrsCmymuMmfx"
      },
      "outputs": [],
      "source": [
        "def clean(data):\n",
        "    for entry in data:\n",
        "        length = len(entry[\"sentence\"])\n",
        "        entry.pop(\"residual_vector\", None)\n",
        "        entry[\"sentence\"] = length / 3000\n",
        "    return data\n",
        "\n",
        "for file_name in os.listdir(new_segment_results_path):\n",
        "    if file_name.endswith(\".json\"):\n",
        "        file_path = os.path.join(new_segment_results_path, file_name)\n",
        "        cleaned_data = clean(load_json(file_path))\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(cleaned_data, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSubF6kvM5vX"
      },
      "source": [
        "Clean the evaluation score, drop explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "DtGEyo2wMxQl"
      },
      "outputs": [],
      "source": [
        "def clean_judge_score(data):\n",
        "    judge = data.get(\"judge_score\", {})\n",
        "    score = judge.get(\"score\", judge.get(\"Score\", None)) if isinstance(judge, dict) else judge\n",
        "    data[\"judge_score\"] = score\n",
        "\n",
        "    return data\n",
        "judge_score = [clean_judge_score(score) for score in judge_score]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZUmwSu9NIq7"
      },
      "source": [
        "## Calculate Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'session_id': '0c44adf9178a443a9fd7a5a2edaeb7c4', 'prompt_code': 'policy'},\n",
              " {'session_id': '2c4b5bda6f6f43ab9ffc990e7cae9693', 'prompt_code': 'policy'},\n",
              " {'session_id': '6df918d72046461a98d275bf3fac31d0', 'prompt_code': 'policy'}]"
            ]
          },
          "execution_count": 222,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_code = []\n",
        "def find_prompt_code(csv_path):\n",
        "    with open(csv_path, mode='r', encoding='utf-8') as file:\n",
        "        csv_reader = csv.DictReader(file)\n",
        "        for row in csv_reader:\n",
        "            session_id = row.get('session_id')\n",
        "            topic = row.get('prompt_code')\n",
        "            if session_id and topic:\n",
        "                prompt_code.append({\n",
        "                    \"session_id\": session_id,\n",
        "                    \"prompt_code\": topic\n",
        "                })\n",
        "    \n",
        "    return prompt_code\n",
        "\n",
        "find_prompt_code(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ziagh-GY_9P3",
        "outputId": "c9a826af-9479-4cc1-f1ea-29c9f61246cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'session_id': '0c44adf9178a443a9fd7a5a2edaeb7c4', 'length': 2893.0},\n",
              " {'session_id': '2c4b5bda6f6f43ab9ffc990e7cae9693', 'length': 3122.0},\n",
              " {'session_id': '6df918d72046461a98d275bf3fac31d0', 'length': 3520.0}]"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "length = []\n",
        "def calculate_length(path):\n",
        "  for file_name in os.listdir(path):\n",
        "      session_id = os.path.splitext(file_name)[0]\n",
        "      if file_name.endswith(\".json\"):\n",
        "          file_path = os.path.join(path, file_name)\n",
        "          data = load_json(file_path)\n",
        "          sentence = data[-1][\"sentence\"] * 3000\n",
        "          length.append({\n",
        "              \"session_id\": session_id,\n",
        "              \"length\": sentence\n",
        "          })\n",
        "  return length\n",
        "\n",
        "calculate_length(new_segment_results_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wesrruWLIJ4B",
        "outputId": "76a941db-b29d-4ce5-cd4c-87ea557575a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'session_id': '0c44adf9178a443a9fd7a5a2edaeb7c4',\n",
              "  'AI_ratio': 0.16197623514696685},\n",
              " {'session_id': '2c4b5bda6f6f43ab9ffc990e7cae9693',\n",
              "  'AI_ratio': 0.2592028433612592},\n",
              " {'session_id': '6df918d72046461a98d275bf3fac31d0',\n",
              "  'AI_ratio': 0.39946342841005367}]"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AI_ratio = []\n",
        "def calculate_AI_ratio(path):\n",
        "  for file_name in os.listdir(path):\n",
        "      ai_num = 0\n",
        "      human_num = 0\n",
        "      session_id = os.path.splitext(file_name)[0]\n",
        "      if file_name.endswith(\".jsonl\"):\n",
        "          file_path = os.path.join(path, file_name)\n",
        "          data = load_json(file_path)\n",
        "          for d in data[\"info\"]:\n",
        "              if d[\"name\"] == \"text-insert\":\n",
        "                  if d[\"eventSource\"] == \"api\":\n",
        "                      ai_num += d[\"count\"]\n",
        "                  else:\n",
        "                      human_num += d[\"count\"]\n",
        "          all = ai_num + human_num\n",
        "          AI_ratio.append({\n",
        "              \"session_id\": session_id,\n",
        "              \"AI_ratio\": ai_num / all\n",
        "          })\n",
        "  return AI_ratio\n",
        "\n",
        "calculate_AI_ratio(new_json_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EaTRgjlLvmz",
        "outputId": "bcb6ba9a-9ee2-4484-808f-f3db44130a84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'session_id': '0c44adf9178a443a9fd7a5a2edaeb7c4',\n",
              "  'sum_semantic_score': 4.138687504488028},\n",
              " {'session_id': '2c4b5bda6f6f43ab9ffc990e7cae9693',\n",
              "  'sum_semantic_score': 5.221882209089281},\n",
              " {'session_id': '6df918d72046461a98d275bf3fac31d0',\n",
              "  'sum_semantic_score': 13.592394537922889}]"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum_semantic_score = []\n",
        "def calculate_sum_semantic_score(path):\n",
        "  for file_name in os.listdir(path):\n",
        "      session_id = os.path.splitext(file_name)[0]\n",
        "      if file_name.endswith(\".json\"):\n",
        "          file_path = os.path.join(path, file_name)\n",
        "          data = load_json(file_path)\n",
        "          score = 0\n",
        "          for d in data:\n",
        "            score += d[\"residual_vector_norm\"]\n",
        "          sum_semantic_score.append({\n",
        "              \"session_id\": session_id,\n",
        "              \"sum_semantic_score\": score\n",
        "          })\n",
        "  return sum_semantic_score\n",
        "\n",
        "calculate_sum_semantic_score(new_segment_results_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KxbM3jSNVvv"
      },
      "source": [
        "## Make Feature File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nak_poe1ZLXd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "yl7DzIF3ZB0U"
      },
      "outputs": [],
      "source": [
        "feature_names = ['prompt_code', 'judge_score', 'length', 'AI_ratio', 'sum_semantic_score']\n",
        "\n",
        "def merge_2_csv_json(feature_names):\n",
        "  data_lists = {\n",
        "     'prompt_code': prompt_code,\n",
        "      'judge_score': judge_score,\n",
        "      'length': length,\n",
        "      'AI_ratio': AI_ratio,\n",
        "      'sum_semantic_score': sum_semantic_score,\n",
        "  }\n",
        "\n",
        "  dfs = [pd.DataFrame(data_lists[feature]) for feature in feature_names]\n",
        "  df = dfs[0]\n",
        "  for other_df in dfs[1:]:\n",
        "      df = df.merge(other_df, on='session_id', how='outer')\n",
        "  output_path_csv = os.path.join(static_dir, f\"dataset/{dataset_name}\", \"fine.csv\")\n",
        "  df.to_csv(output_path_csv, index=False)\n",
        "  output_path_json = os.path.join(static_dir, f\"dataset/{dataset_name}\", \"fine.json\")\n",
        "  data = df.to_dict(orient='records')\n",
        "  with open(output_path_json, \"w\", encoding=\"utf-8\") as f:\n",
        "      json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "merge_2_csv_json(feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_name_path = os.path.join(static_dir, \"dataset_name.json\")\n",
        "data_name = load_json(dataset_name_path)\n",
        "data_name.append(dataset_name)\n",
        "with open(dataset_name_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data_name, f, ensure_ascii=False, indent=4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
